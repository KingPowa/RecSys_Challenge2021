{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!kaggle competitions download -c recommender-system-2021-challenge-polimi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "dataFile = zipfile.ZipFile(\"recommender-system-2021-challenge-polimi.zip\")\n",
    "dataFile.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "URM_path = 'data/data_train.csv'\n",
    "URM_all_dataframe = pd.read_csv(filepath_or_buffer=URM_path, \n",
    "                                sep=\",\",\n",
    "                                dtype={0:int, 1:int, 2:float})\n",
    "URM_all_dataframe.columns = [\"UserID\", \"ItemID\", \"Interaction\"]\n",
    "URM_all_dataframe.head(n=10)\n",
    "\n",
    "ICM_genre_path = 'data/data_ICM_genre.csv'\n",
    "ICM_genre_all_dataframe = pd.read_csv(filepath_or_buffer=ICM_genre_path, \n",
    "                                sep=\",\",\n",
    "                                dtype={0:int, 1:int, 2:float})\n",
    "ICM_genre_all_dataframe.columns = [\"ItemID\", \"GenreID\", \"Match\"]\n",
    "ICM_genre_all_dataframe.head(n=10)\n",
    "\n",
    "ICM_subgenre_path = 'data/data_ICM_subgenre.csv'\n",
    "ICM_subgenre_all_dataframe = pd.read_csv(filepath_or_buffer=ICM_subgenre_path, \n",
    "                                sep=\",\",\n",
    "                                dtype={0:int, 1:int, 2:float})\n",
    "ICM_subgenre_all_dataframe.columns = [\"ItemID\", \"SubgenreID\", \"Match\"]\n",
    "ICM_subgenre_all_dataframe.head(n=10)\n",
    "\n",
    "ICM_channel_path = 'data/data_ICM_channel.csv'\n",
    "ICM_channel_all_dataframe = pd.read_csv(filepath_or_buffer=ICM_channel_path, \n",
    "                                sep=\",\",\n",
    "                                dtype={0:int, 1:int, 2:float})\n",
    "ICM_channel_all_dataframe.columns = [\"ItemID\", \"ChannelID\", \"Match\"]\n",
    "ICM_channel_all_dataframe.head(n=10)\n",
    "\n",
    "ICM_event_path = 'data/data_ICM_event.csv'\n",
    "ICM_event_all_dataframe = pd.read_csv(filepath_or_buffer=ICM_event_path, \n",
    "                                sep=\",\",\n",
    "                                dtype={0:int, 1:int, 2:float})\n",
    "ICM_event_all_dataframe.columns = [\"ItemID\", \"EpisodeID\", \"Match\"]\n",
    "ICM_event_all_dataframe.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_subgenre_all_dataframe.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genreID_unique = ICM_genre_all_dataframe[\"GenreID\"].unique()\n",
    "itemID_unique = ICM_genre_all_dataframe[\"ItemID\"].unique()\n",
    "n_genres = len(genreID_unique)\n",
    "n_items = len(itemID_unique)\n",
    "n_matches = len(ICM_genre_all_dataframe)\n",
    "\n",
    "print (\"Number of items\\t {}, Number of genres\\t {}\".format(n_items, n_genres))\n",
    "print (\"Max ID items\\t {}, Max Id genres\\t {}\\n\".format(max(itemID_unique), max(genreID_unique)))\n",
    "print (\"Average matches per genre {:.2f}\".format(n_matches/n_genres))\n",
    "print (\"Average matches per item {:.2f}\\n\".format(n_matches/n_items))\n",
    "\n",
    "print (\"Sparsity {:.2f} %\".format((1-float(n_matches)/(n_items*n_users))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To COO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "URM_all = sps.coo_matrix((URM_all_dataframe[\"Interaction\"].values, \n",
    "                          (URM_all_dataframe[\"UserID\"].values, URM_all_dataframe[\"ItemID\"].values)))\n",
    "\n",
    "# URM_all.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_genre_all = sps.coo_matrix((ICM_genre_all_dataframe[\"Match\"].values, \n",
    "                          (ICM_genre_all_dataframe[\"ItemID\"].values, ICM_genre_all_dataframe[\"GenreID\"].values)))\n",
    "\n",
    "# ICM_genre_all.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_subgenre_all = sps.coo_matrix((ICM_subgenre_all_dataframe[\"Match\"].values, \n",
    "                          (ICM_subgenre_all_dataframe[\"ItemID\"].values, ICM_subgenre_all_dataframe[\"SubgenreID\"].values)))\n",
    "\n",
    "# ICM_subgenre_all.tocsr().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_channel_all = sps.coo_matrix((ICM_channel_all_dataframe[\"Match\"].values, \n",
    "                          (ICM_channel_all_dataframe[\"ItemID\"].values, ICM_channel_all_dataframe[\"ChannelID\"].values)))\n",
    "\n",
    "# ICM_channel_all.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_event_all = sps.coo_matrix((ICM_event_all_dataframe[\"Match\"].values, \n",
    "                          (ICM_event_all_dataframe[\"ItemID\"].values, ICM_event_all_dataframe[\"EpisodeID\"].values)))\n",
    "\n",
    "# ICM_subgenre_all.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13650x18059 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1057044 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "train_test_split = 0.80\n",
    "\n",
    "n_interactions = URM_all.nnz\n",
    "\n",
    "\n",
    "train_mask = np.random.choice([True,False], n_interactions, p=[train_test_split, 1-train_test_split])\n",
    "train_mask\n",
    "\n",
    "URM_train = sps.csr_matrix((URM_all.data[train_mask],\n",
    "                            (URM_all.row[train_mask], URM_all.col[train_mask])))\n",
    "\n",
    "val_mask = np.logical_not(train_mask)\n",
    "\n",
    "URM_val = sps.csr_matrix((URM_all.data[val_mask],\n",
    "                            (URM_all.row[val_mask], URM_all.col[val_mask])))\n",
    "\n",
    "URM_train\n",
    "URM_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 124\n",
    "# inefficient way (creates new CSR)\n",
    "# relevant_items = URM_val[user_id].indices\n",
    "relevant_items = URM_val.indices[URM_val.indptr[user_id]:URM_val.indptr[user_id+1]]\n",
    "relevant_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant) #True == 1\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    # e.g. cumsum on [1,2,3] gets [1,3,6]\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass as paramether the recommender class\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, at=10):\n",
    "    \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "\n",
    "    for user_id in range(URM_test.shape[0]):\n",
    "\n",
    "        relevant_items = URM_test.indices[URM_test.indptr[user_id]:URM_test.indptr[user_id+1]]\n",
    "        # take the user row and get the cols of non-zero vals\n",
    "        # uses indptr to optimise\n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = recommender_object.recommend(user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "            \n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRecommender(object):\n",
    "\n",
    "    def fit(self, URM_train):\n",
    "           \n",
    "        self.n_items = URM_train.shape[1]\n",
    "    \n",
    "    \n",
    "    def recommend(self, user_id, at=10):\n",
    "    \n",
    "        recommended_items = np.random.choice(self.n_items, at)\n",
    "\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomRecommender = RandomRecommender()\n",
    "randomRecommender.fit(URM_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_algorithm(URM_val, randomRecommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 most popular items\n",
    "item_popularity = np.ediff1d(URM_all.tocsc().indptr)\n",
    "popular_items = np.argsort(item_popularity)\n",
    "popular_items = np.flip(popular_items, axis = 0)\n",
    "\n",
    "popular_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopPopRecommender(object):\n",
    "\n",
    "    def fit(self, URM_train):\n",
    "        \n",
    "        self.URM_train = URM_train\n",
    "\n",
    "        item_popularity = np.ediff1d(URM_all.tocsc().indptr)\n",
    "\n",
    "        # We are not interested in sorting the popularity value,\n",
    "        # but to order the items according to it\n",
    "        self.popular_items = np.argsort(item_popularity)\n",
    "        self.popular_items = np.flip(self.popular_items, axis = 0)\n",
    "    \n",
    "    \n",
    "    def recommend(self, user_id, at=10, remove_seen=True):\n",
    "        # obvs we are removing the seen from the specific user recomm, not the general one! it would get super small otherwise\n",
    "\n",
    "        if remove_seen:\n",
    "            seen_items = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id+1]]\n",
    "            # always using the internal structures\n",
    "            \n",
    "            unseen_items_mask = np.in1d(self.popular_items, seen_items,\n",
    "                                        assume_unique=True, invert = True)\n",
    "\n",
    "            unseen_items = self.popular_items[unseen_items_mask]\n",
    "\n",
    "            recommended_items = unseen_items[0:at]\n",
    "\n",
    "        else:\n",
    "            recommended_items = self.popular_items[0:at]\n",
    "            \n",
    "\n",
    "        return recommended_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topPopRecommender = TopPopRecommender()\n",
    "topPopRecommender.fit(URM_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_id in range(10):\n",
    "    print(topPopRecommender.recommend(user_id, at=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_algorithm(URM_val, topPopRecommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Recommenders.Similarity.Compute_Similarity_Python import Compute_Similarity_Python\n",
    "\n",
    "class ItemKNNCBFRecommender(object):\n",
    "    \n",
    "    def __init__(self, URM, ICM):\n",
    "        self.URM = URM\n",
    "        self.ICM = ICM.tocsr()\n",
    "        \n",
    "            \n",
    "    def fit(self, topK=50, shrink=100, normalize = True, similarity = \"cosine\"):\n",
    "        \n",
    "        similarity_object = Compute_Similarity_Python(self.ICM.T, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "        \n",
    "        self.W_sparse = similarity_object.compute_similarity()\n",
    "\n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenreRecommender = ItemKNNCBFRecommender(URM_train, ICM_genre_all)\n",
    "GenreRecommender.fit(shrink=0.0, topK=50)\n",
    "\n",
    "evaluate_algorithm(URM_val, GenreRecommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "evaluator_val = EvaluatorHoldout(URM_val, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 23/10/17\n",
    "\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "from Recommenders.Recommender_utils import check_matrix\n",
    "from Recommenders.BaseCBFRecommender import BaseItemCBFRecommender\n",
    "from Recommenders.BaseSimilarityMatrixRecommender import BaseItemSimilarityMatrixRecommender\n",
    "from Recommenders.IR_feature_weighting import okapi_BM_25, TF_IDF\n",
    "import numpy as np\n",
    "\n",
    "from Recommenders.Similarity.Compute_Similarity import Compute_Similarity\n",
    "\n",
    "\n",
    "class ItemKNNCBFRecommender(BaseItemCBFRecommender, BaseItemSimilarityMatrixRecommender):\n",
    "    \"\"\" ItemKNN recommender\"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"ItemKNNCBFRecommender\"\n",
    "\n",
    "    FEATURE_WEIGHTING_VALUES = [\"BM25\", \"TF-IDF\", \"none\"]\n",
    "\n",
    "    def __init__(self, URM_train, ICM_train, verbose = True):\n",
    "        super(ItemKNNCBFRecommender, self).__init__(URM_train, ICM_train, verbose = verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, topK=50, shrink=100, similarity='cosine', normalize=True, feature_weighting = \"none\", ICM_bias = None, **similarity_args):\n",
    "\n",
    "        self.topK = topK\n",
    "        self.shrink = shrink\n",
    "\n",
    "        if feature_weighting not in self.FEATURE_WEIGHTING_VALUES:\n",
    "            raise ValueError(\"Value for 'feature_weighting' not recognized. Acceptable values are {}, provided was '{}'\".format(self.FEATURE_WEIGHTING_VALUES, feature_weighting))\n",
    "\n",
    "        if ICM_bias is not None:\n",
    "            self.ICM_train.data += ICM_bias\n",
    "\n",
    "        if feature_weighting == \"BM25\":\n",
    "            self.ICM_train = self.ICM_train.astype(np.float32)\n",
    "            self.ICM_train = okapi_BM_25(self.ICM_train)\n",
    "\n",
    "        elif feature_weighting == \"TF-IDF\":\n",
    "            self.ICM_train = self.ICM_train.astype(np.float32)\n",
    "            self.ICM_train = TF_IDF(self.ICM_train)\n",
    "\n",
    "\n",
    "        similarity = Compute_Similarity(self.ICM_train.T, shrink=shrink, topK=topK, normalize=normalize, similarity = similarity, **similarity_args)\n",
    "\n",
    "        self.W_sparse = similarity.compute_similarity()\n",
    "        self.W_sparse = check_matrix(self.W_sparse, format='csr')\n",
    "        \n",
    "    def fit_and_ret(self, topK=50, shrink=100, similarity='cosine', normalize=True, feature_weighting = \"none\", ICM_bias = None, **similarity_args):\n",
    "\n",
    "        self.topK = topK\n",
    "        self.shrink = shrink\n",
    "\n",
    "        if feature_weighting not in self.FEATURE_WEIGHTING_VALUES:\n",
    "            raise ValueError(\"Value for 'feature_weighting' not recognized. Acceptable values are {}, provided was '{}'\".format(self.FEATURE_WEIGHTING_VALUES, feature_weighting))\n",
    "\n",
    "        if ICM_bias is not None:\n",
    "            self.ICM_train.data += ICM_bias\n",
    "\n",
    "        if feature_weighting == \"BM25\":\n",
    "            self.ICM_train = self.ICM_train.astype(np.float32)\n",
    "            self.ICM_train = okapi_BM_25(self.ICM_train)\n",
    "\n",
    "        elif feature_weighting == \"TF-IDF\":\n",
    "            self.ICM_train = self.ICM_train.astype(np.float32)\n",
    "            self.ICM_train = TF_IDF(self.ICM_train)\n",
    "\n",
    "\n",
    "        similarity = Compute_Similarity(self.ICM_train.T, shrink=shrink, topK=topK, normalize=normalize, similarity = similarity, **similarity_args)\n",
    "\n",
    "        self.W_sparse = similarity.compute_similarity()\n",
    "        self.W_sparse = check_matrix(self.W_sparse, format='csr')\n",
    "        return self.W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Recommenders.KNN.ItemKNNCBFRecommender import ItemKNNCBFRecommender\n",
    "\n",
    "\n",
    "x_tick = [1, 5, 10, 30, 50]\n",
    "MAP_per_k = []\n",
    "\n",
    "for topK in x_tick:\n",
    "    \n",
    "    recommender = ItemKNNCBFRecommender(URM_train, ICM_genre_all.tocsr())\n",
    "    similarity_matrix = recommender.fit_and_ret(shrink=0.5, topK=topK, feature_weighting = 'BM25')\n",
    "    print(similarity_matrix.data[:10])\n",
    "    \n",
    "    result_df, _ = evaluator_val.evaluateRecommender(recommender)\n",
    "    \n",
    "    MAP_per_k.append(result_df.loc[10][\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "pyplot.plot(x_tick, MAP_per_k)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('TopK')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgenre Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Ignoring 13645 ( 0.0%) Users that have less than 1 test interactions\n",
      "ItemKNNCBFRecommender: ICM Detected 487 ( 2.7%) items with no features.\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 18059 (100.0%), 7732.31 column/sec. Elapsed time 2.34 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 6.89 sec. Users per second: 1980\n",
      "ItemKNNCBFRecommender: ICM Detected 487 ( 2.7%) items with no features.\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 18059 (100.0%), 7669.86 column/sec. Elapsed time 2.35 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 7.07 sec. Users per second: 1930\n",
      "ItemKNNCBFRecommender: ICM Detected 487 ( 2.7%) items with no features.\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 18059 (100.0%), 7709.15 column/sec. Elapsed time 2.34 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 7.21 sec. Users per second: 1893\n",
      "ItemKNNCBFRecommender: ICM Detected 487 ( 2.7%) items with no features.\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 18059 (100.0%), 7709.19 column/sec. Elapsed time 2.34 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 7.34 sec. Users per second: 1858\n",
      "ItemKNNCBFRecommender: ICM Detected 487 ( 2.7%) items with no features.\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 18059 (100.0%), 7640.68 column/sec. Elapsed time 2.36 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 7.45 sec. Users per second: 1832\n"
     ]
    }
   ],
   "source": [
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "evaluator_val = EvaluatorHoldout(URM_val, cutoff_list=[10])\n",
    "\n",
    "from Recommenders.KNN.ItemKNNCBFRecommender import ItemKNNCBFRecommender\n",
    "\n",
    "x_tick = [10, 12, 14, 18, 20]\n",
    "MAP_per_k = []\n",
    "\n",
    "for topK in x_tick:\n",
    "    \n",
    "    recommender = ItemKNNCBFRecommender(URM_train, ICM_subgenre_all.tocsr())\n",
    "    recommender.fit(shrink=0.0, topK=topK)\n",
    "    \n",
    "    result_df, _ = evaluator_val.evaluateRecommender(recommender)\n",
    "    \n",
    "    MAP_per_k.append(result_df.loc[10][\"MAP\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shrink tuning\n",
    "\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "evaluator_val = EvaluatorHoldout(URM_val, cutoff_list=[10])\n",
    "\n",
    "from Recommenders.KNN.ItemKNNCBFRecommender import ItemKNNCBFRecommender\n",
    "\n",
    "x_tick = [0.0, 0.5, 1.5, 5.0, 50.0]\n",
    "MAP_per_k = []\n",
    "\n",
    "for topK in x_tick:\n",
    "    \n",
    "    recommender = ItemKNNCBFRecommender(URM_train, ICM_subgenre_all.tocsr())\n",
    "    recommender.fit(shrink=topK, topK=18)\n",
    "    \n",
    "    result_df, _ = evaluator_val.evaluateRecommender(recommender)\n",
    "    \n",
    "    MAP_per_k.append(result_df.loc[10][\"MAP\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "pyplot.plot(x_tick, MAP_per_k)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('TopK')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE ALGORITHM HERE\n",
    "recommender = TopPopRecommender() # <-----\n",
    "K = 10\n",
    "\n",
    "recommender.fit(URM_all.tocsr())\n",
    "\n",
    "user_test_path = 'data/data_target_users_test.csv'\n",
    "user_test_dataframe = pd.read_csv(filepath_or_buffer=user_test_path,\n",
    "                                sep=\",\",\n",
    "                                dtype={0:int})\n",
    "\n",
    "subm_set = user_test_dataframe.to_numpy().T[0]\n",
    "\n",
    "\n",
    "subm_res = {\"user_id\":[], \"item_list\":[]}\n",
    "\n",
    "for user_id in subm_set:\n",
    "    subm_res[\"user_id\"].append(user_id)\n",
    "    res = recommender.recommend(user_id, at=K)\n",
    "    res = ' '.join(map(str, res))\n",
    "    if user_id < 3:\n",
    "        print(user_id)\n",
    "        print(res)\n",
    "    subm_res[\"item_list\"].append(res)\n",
    "\n",
    "\n",
    "# print(subm_res)\n",
    "\n",
    "submission = pd.DataFrame.from_dict(subm_res)\n",
    "# submission\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "\n",
    "submission.to_csv('subs/submission {:%Y_%m_%d %H_%M_%S}.csv'.format(now), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
