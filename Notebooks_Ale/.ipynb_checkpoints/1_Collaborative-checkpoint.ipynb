{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../RecSysRep/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Basics.Load as ld\n",
    "\n",
    "URM_all, ICM_genre_all, ICM_subgenre_all, ICM_channel_all, ICM_event_all = ld.getCOOs()\n",
    "# URM_train, URM_val = ld.getSplit(URM_train_val, 5678, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 8 (0.06 %) of 13650 users have no sampled items\n",
      "Warning: 9 (0.07 %) of 13650 users have no sampled items\n",
      "EvaluatorHoldout: Ignoring 13641 ( 0.1%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 13642 ( 0.1%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "\n",
    "URM_train_validation, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.8)\n",
    "URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train_validation, train_percentage = 0.8)\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Using cached scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (from scikit-optimize) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (from scikit-optimize) (1.0.1)\n",
      "Collecting pyaml>=16.9\n",
      "  Using cached pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (from scikit-optimize) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (from scikit-optimize) (1.20.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (5.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.1.0)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n"
     ]
    }
   ],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "hyperparameters_range_dictionary = {\n",
    "    \"topK\": Integer(200, 800),\n",
    "    \"shrink\": Integer(0, 100),\n",
    "    \"similarity\": Categorical([\"cosine\"]),\n",
    "    \"normalize\": Categorical([True]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Recommenders.KNN.ItemKNNCFRecommender import ItemKNNCFRecommender\n",
    "from HyperparameterTuning.SearchBayesianSkopt import SearchBayesianSkopt\n",
    "\n",
    "recommender_class = ItemKNNCFRecommender\n",
    "\n",
    "hyperparameterSearch = SearchBayesianSkopt(recommender_class,\n",
    "                                         evaluator_validation=evaluator_validation,\n",
    "                                         evaluator_test=evaluator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterTuning.SearchAbstractClass import SearchInputRecommenderArgs\n",
    "  \n",
    "recommender_input_args = SearchInputRecommenderArgs(\n",
    "    CONSTRUCTOR_POSITIONAL_ARGS = [URM_train],     # For a CBF model simply put [URM_train, ICM_train]\n",
    "    CONSTRUCTOR_KEYWORD_ARGS = {},\n",
    "    FIT_POSITIONAL_ARGS = [],\n",
    "    FIT_KEYWORD_ARGS = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_input_args_last_test = SearchInputRecommenderArgs(\n",
    "    CONSTRUCTOR_POSITIONAL_ARGS = [URM_train_validation],     # For a CBF model simply put [URM_train_validation, ICM_train]\n",
    "    CONSTRUCTOR_KEYWORD_ARGS = {},\n",
    "    FIT_POSITIONAL_ARGS = [],\n",
    "    FIT_KEYWORD_ARGS = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_folder_path = \"result_experiments/1_Collaborative\"\n",
    "\n",
    "# If directory does not exist, create\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "    \n",
    "n_cases = 50  # using 10 as an example\n",
    "n_random_starts = int(n_cases*0.3)\n",
    "metric_to_optimize = \"MAP\"   \n",
    "cutoff_to_optimize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.9.3 in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (from tables) (1.20.1)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in c:\\users\\marti\\miniconda3\\envs\\recsys\\lib\\site-packages (from tables) (2.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "SearchBayesianSkopt: Testing config: {'topK': 64, 'shrink': 497, 'similarity': 'cosine', 'normalize': False}\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 18059 (100.0%), 610.77 column/sec. Elapsed time 29.57 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3c083f7dc738>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m hyperparameterSearch.search(recommender_input_args,\n\u001b[0m\u001b[0;32m      2\u001b[0m                        \u001b[0mrecommender_input_args_last_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecommender_input_args_last_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                        \u001b[0mhyperparameter_search_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparameters_range_dictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                        \u001b[0mn_cases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_cases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                        \u001b[0mn_random_starts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_random_starts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Lab Recommender\\Challenge\\HyperparameterTuning\\SearchBayesianSkopt.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, recommender_input_args, hyperparameter_search_space, metric_to_optimize, cutoff_to_optimize, n_cases, n_random_starts, output_folder_path, output_file_name_root, save_model, save_metadata, resume_from_saved, recommender_input_args_last_test, evaluate_on_test, max_total_time)\u001b[0m\n\u001b[0;32m    336\u001b[0m                 \u001b[1;31m# of \"Searching for the next optimal point\". This may be due to a bug in the print rather than the underlying process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m                 \u001b[1;31m# https://github.com/scikit-optimize/scikit-optimize/issues/949\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m                 self.result = gp_minimize(self._objective_function_list_input,\n\u001b[0m\u001b[0;32m    339\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparams_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                                           \u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\RecSys\\lib\\site-packages\\skopt\\optimizer\\gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    257\u001b[0m             noise=noise)\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m     return base_minimize(\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0macq_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0macq_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\RecSys\\lib\\site-packages\\skopt\\optimizer\\base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         \u001b[0mnext_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Lab Recommender\\Challenge\\HyperparameterTuning\\SearchBayesianSkopt.py\u001b[0m in \u001b[0;36m_objective_function_list_input\u001b[1;34m(self, current_fit_hyperparameters_list_of_values)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mcurrent_fit_hyperparameters_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparams_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_fit_hyperparameters_list_of_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_objective_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_fit_hyperparameters_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;31m# The search can only progress if there is at least a valid config in the initial random start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Lab Recommender\\Challenge\\HyperparameterTuning\\SearchAbstractClass.py\u001b[0m in \u001b[0;36m_objective_function\u001b[1;34m(self, current_fit_hyperparameters_dict)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[1;31m# If getting a interrupt, terminate without saving the exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Lab Recommender\\Challenge\\HyperparameterTuning\\SearchAbstractClass.py\u001b[0m in \u001b[0;36m_objective_function\u001b[1;34m(self, current_fit_hyperparameters_dict)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[0mwas_already_evaluated_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwas_already_evaluated_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_was_already_evaluated_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_fit_hyperparameters_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m             \u001b[0mresult_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecommender_instance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate_on_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_fit_hyperparameters_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwas_already_evaluated_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwas_already_evaluated_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[0mresult_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cutoff_to_optimize\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Lab Recommender\\Challenge\\HyperparameterTuning\\SearchAbstractClass.py\u001b[0m in \u001b[0;36m_evaluate_on_validation\u001b[1;34m(self, current_fit_hyperparameters, was_already_evaluated_flag, was_already_evaluated_index)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# Evaluate recommender and get results for the first cutoff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mresult_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluator_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluateRecommender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommender_instance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[0mevaluation_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Lab Recommender\\Challenge\\Evaluation\\Evaluator.py\u001b[0m in \u001b[0;36mevaluateRecommender\u001b[1;34m(self, recommender_object)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_users_evaluated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mresults_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_evaluation_on_selected_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommender_object\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musers_to_evaluate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Lab Recommender\\Challenge\\Evaluation\\Evaluator.py\u001b[0m in \u001b[0;36m_run_evaluation_on_selected_users\u001b[1;34m(self, recommender_object, users_to_evaluate, block_size)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[1;31m# Compute predictions for a batch of users using vectorization, much more efficient than computing it one at a time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m             recommended_items_batch_list, scores_batch = recommender_object.recommend(test_user_batch_array,\n\u001b[0m\u001b[0;32m    482\u001b[0m                                                                       \u001b[0mremove_seen_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexclude_seen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m                                                                       \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Lab Recommender\\Challenge\\Recommenders\\BaseRecommender.py\u001b[0m in \u001b[0;36mrecommend\u001b[1;34m(self, user_id_array, cutoff, remove_seen_flag, items_to_compute, remove_top_pop_flag, remove_custom_items_flag, return_scores)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# relevant_items_partition is block_size x cutoff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mrelevant_items_partition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mscores_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;31m# Get original value and sort it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparameterSearch.search(recommender_input_args,\n",
    "                       recommender_input_args_last_test = recommender_input_args_last_test,\n",
    "                       hyperparameter_search_space = hyperparameters_range_dictionary,\n",
    "                       n_cases = n_cases,\n",
    "                       n_random_starts = n_random_starts,\n",
    "                       save_model = \"last\",\n",
    "                       output_folder_path = output_folder_path, # Where to save the results\n",
    "                       output_file_name_root = recommender_class.RECOMMENDER_NAME, # How to call the files\n",
    "                       metric_to_optimize = metric_to_optimize,\n",
    "                       cutoff_to_optimize = cutoff_to_optimize,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['algorithm_name_recommender', 'algorithm_name_search', 'cutoff_to_optimize', 'exception_list', 'hyperparameters_best', 'hyperparameters_best_index', 'hyperparameters_df', 'metric_to_optimize', 'result_on_last', 'result_on_test_best', 'result_on_test_df', 'result_on_validation_best', 'result_on_validation_df', 'time_df', 'time_on_last_df', 'time_on_test_avg', 'time_on_test_total', 'time_on_train_avg', 'time_on_train_total', 'time_on_validation_avg', 'time_on_validation_total'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Recommenders.DataIO import DataIO\n",
    "\n",
    "data_loader = DataIO(folder_path = output_folder_path)\n",
    "search_metadata = data_loader.load_data(recommender_class.RECOMMENDER_NAME + \"_metadata.zip\")\n",
    "\n",
    "search_metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topK': 324, 'shrink': 0, 'similarity': 'cosine', 'normalize': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_df = search_metadata[\"hyperparameters_best\"]\n",
    "hyperparameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_ITEM_CORRECT</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_CORRECT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>10</th>\n",
       "      <td>0.216702</td>\n",
       "      <td>0.218582</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>0.10627</td>\n",
       "      <td>0.107255</td>\n",
       "      <td>0.451879</td>\n",
       "      <td>0.227804</td>\n",
       "      <td>0.078539</td>\n",
       "      <td>0.884207</td>\n",
       "      <td>0.690058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016446</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>5.731718</td>\n",
       "      <td>0.972642</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.462668</td>\n",
       "      <td>3.488735</td>\n",
       "      <td>0.030756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <td>0.198051</td>\n",
       "      <td>0.199712</td>\n",
       "      <td>0.042484</td>\n",
       "      <td>0.097737</td>\n",
       "      <td>0.098644</td>\n",
       "      <td>0.436063</td>\n",
       "      <td>0.211581</td>\n",
       "      <td>0.069961</td>\n",
       "      <td>0.851228</td>\n",
       "      <td>0.649777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.850916</td>\n",
       "      <td>0.00121</td>\n",
       "      <td>4.649879</td>\n",
       "      <td>0.955118</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>0.375341</td>\n",
       "      <td>3.776113</td>\n",
       "      <td>0.030215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>10</th>\n",
       "      <td>0.19528</td>\n",
       "      <td>0.19696</td>\n",
       "      <td>0.041702</td>\n",
       "      <td>0.096592</td>\n",
       "      <td>0.097548</td>\n",
       "      <td>0.433894</td>\n",
       "      <td>0.209346</td>\n",
       "      <td>0.068727</td>\n",
       "      <td>0.845071</td>\n",
       "      <td>0.644415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.844762</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>4.555582</td>\n",
       "      <td>0.953094</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.367729</td>\n",
       "      <td>3.793276</td>\n",
       "      <td>0.030177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>10</th>\n",
       "      <td>0.194899</td>\n",
       "      <td>0.196596</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>0.09652</td>\n",
       "      <td>0.097471</td>\n",
       "      <td>0.434693</td>\n",
       "      <td>0.209191</td>\n",
       "      <td>0.06854</td>\n",
       "      <td>0.844119</td>\n",
       "      <td>0.64473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.84381</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>4.544407</td>\n",
       "      <td>0.952807</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.366828</td>\n",
       "      <td>3.795377</td>\n",
       "      <td>0.030173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>10</th>\n",
       "      <td>0.211513</td>\n",
       "      <td>0.213258</td>\n",
       "      <td>0.046039</td>\n",
       "      <td>0.104431</td>\n",
       "      <td>0.105312</td>\n",
       "      <td>0.448648</td>\n",
       "      <td>0.223676</td>\n",
       "      <td>0.075619</td>\n",
       "      <td>0.873067</td>\n",
       "      <td>0.681465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.872747</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>5.23377</td>\n",
       "      <td>0.965609</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.422473</td>\n",
       "      <td>3.640208</td>\n",
       "      <td>0.030463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.2158</td>\n",
       "      <td>0.047066</td>\n",
       "      <td>0.104902</td>\n",
       "      <td>0.105802</td>\n",
       "      <td>0.448261</td>\n",
       "      <td>0.225132</td>\n",
       "      <td>0.07716</td>\n",
       "      <td>0.880322</td>\n",
       "      <td>0.682769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>5.534669</td>\n",
       "      <td>0.970676</td>\n",
       "      <td>0.008401</td>\n",
       "      <td>0.446762</td>\n",
       "      <td>3.573563</td>\n",
       "      <td>0.030552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>10</th>\n",
       "      <td>0.195075</td>\n",
       "      <td>0.196788</td>\n",
       "      <td>0.041639</td>\n",
       "      <td>0.096547</td>\n",
       "      <td>0.09751</td>\n",
       "      <td>0.4344</td>\n",
       "      <td>0.209266</td>\n",
       "      <td>0.068629</td>\n",
       "      <td>0.843313</td>\n",
       "      <td>0.64457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.843004</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>4.547455</td>\n",
       "      <td>0.952883</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.367074</td>\n",
       "      <td>3.794919</td>\n",
       "      <td>0.030174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <td>0.209036</td>\n",
       "      <td>0.210734</td>\n",
       "      <td>0.045319</td>\n",
       "      <td>0.099735</td>\n",
       "      <td>0.100452</td>\n",
       "      <td>0.425312</td>\n",
       "      <td>0.216558</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.864492</td>\n",
       "      <td>0.64878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.864176</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>6.265358</td>\n",
       "      <td>0.980976</td>\n",
       "      <td>0.013698</td>\n",
       "      <td>0.505744</td>\n",
       "      <td>3.355526</td>\n",
       "      <td>0.030893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>10</th>\n",
       "      <td>0.195346</td>\n",
       "      <td>0.197013</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.096606</td>\n",
       "      <td>0.097541</td>\n",
       "      <td>0.434797</td>\n",
       "      <td>0.209464</td>\n",
       "      <td>0.068729</td>\n",
       "      <td>0.845071</td>\n",
       "      <td>0.64515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.844762</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>4.557139</td>\n",
       "      <td>0.953081</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.367855</td>\n",
       "      <td>3.79281</td>\n",
       "      <td>0.030179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <td>0.200007</td>\n",
       "      <td>0.201718</td>\n",
       "      <td>0.043175</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.099355</td>\n",
       "      <td>0.437053</td>\n",
       "      <td>0.213121</td>\n",
       "      <td>0.07102</td>\n",
       "      <td>0.853939</td>\n",
       "      <td>0.653221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.853626</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>4.887093</td>\n",
       "      <td>0.959726</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.394489</td>\n",
       "      <td>3.73683</td>\n",
       "      <td>0.030289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>10</th>\n",
       "      <td>0.194936</td>\n",
       "      <td>0.196647</td>\n",
       "      <td>0.041631</td>\n",
       "      <td>0.096509</td>\n",
       "      <td>0.097463</td>\n",
       "      <td>0.434455</td>\n",
       "      <td>0.209166</td>\n",
       "      <td>0.068609</td>\n",
       "      <td>0.843679</td>\n",
       "      <td>0.644427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.84337</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>4.546945</td>\n",
       "      <td>0.952868</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.367032</td>\n",
       "      <td>3.795023</td>\n",
       "      <td>0.030174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>10</th>\n",
       "      <td>0.207952</td>\n",
       "      <td>0.209632</td>\n",
       "      <td>0.04509</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.100233</td>\n",
       "      <td>0.424614</td>\n",
       "      <td>0.215836</td>\n",
       "      <td>0.074111</td>\n",
       "      <td>0.861781</td>\n",
       "      <td>0.64803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01844</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.861465</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>6.390841</td>\n",
       "      <td>0.982446</td>\n",
       "      <td>0.01491</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>3.308902</td>\n",
       "      <td>0.030971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>10</th>\n",
       "      <td>0.209527</td>\n",
       "      <td>0.211256</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>0.103507</td>\n",
       "      <td>0.104434</td>\n",
       "      <td>0.446876</td>\n",
       "      <td>0.221972</td>\n",
       "      <td>0.074677</td>\n",
       "      <td>0.869476</td>\n",
       "      <td>0.677103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009026</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.869158</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>5.108489</td>\n",
       "      <td>0.963717</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>0.412361</td>\n",
       "      <td>3.677017</td>\n",
       "      <td>0.030383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <th>10</th>\n",
       "      <td>0.207409</td>\n",
       "      <td>0.209152</td>\n",
       "      <td>0.044869</td>\n",
       "      <td>0.102473</td>\n",
       "      <td>0.103445</td>\n",
       "      <td>0.444252</td>\n",
       "      <td>0.220052</td>\n",
       "      <td>0.073777</td>\n",
       "      <td>0.866325</td>\n",
       "      <td>0.671676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007033</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.866007</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>4.991772</td>\n",
       "      <td>0.961863</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.402939</td>\n",
       "      <td>3.70647</td>\n",
       "      <td>0.030324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>10</th>\n",
       "      <td>0.209718</td>\n",
       "      <td>0.211404</td>\n",
       "      <td>0.045512</td>\n",
       "      <td>0.103244</td>\n",
       "      <td>0.104119</td>\n",
       "      <td>0.444162</td>\n",
       "      <td>0.221585</td>\n",
       "      <td>0.074793</td>\n",
       "      <td>0.868597</td>\n",
       "      <td>0.674305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009026</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.868278</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>5.127738</td>\n",
       "      <td>0.964038</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.413914</td>\n",
       "      <td>3.673055</td>\n",
       "      <td>0.030387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>10</th>\n",
       "      <td>0.20911</td>\n",
       "      <td>0.210995</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.103278</td>\n",
       "      <td>0.104252</td>\n",
       "      <td>0.446462</td>\n",
       "      <td>0.221753</td>\n",
       "      <td>0.075116</td>\n",
       "      <td>0.869623</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.869304</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>5.138025</td>\n",
       "      <td>0.964181</td>\n",
       "      <td>0.006371</td>\n",
       "      <td>0.414745</td>\n",
       "      <td>3.670592</td>\n",
       "      <td>0.030387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>10</th>\n",
       "      <td>0.217435</td>\n",
       "      <td>0.219206</td>\n",
       "      <td>0.047885</td>\n",
       "      <td>0.105902</td>\n",
       "      <td>0.106684</td>\n",
       "      <td>0.444706</td>\n",
       "      <td>0.227002</td>\n",
       "      <td>0.078486</td>\n",
       "      <td>0.882814</td>\n",
       "      <td>0.684076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.882491</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>5.795739</td>\n",
       "      <td>0.972831</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>0.467836</td>\n",
       "      <td>3.45587</td>\n",
       "      <td>0.030911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <th>10</th>\n",
       "      <td>0.219384</td>\n",
       "      <td>0.22123</td>\n",
       "      <td>0.048834</td>\n",
       "      <td>0.106602</td>\n",
       "      <td>0.107462</td>\n",
       "      <td>0.446143</td>\n",
       "      <td>0.228548</td>\n",
       "      <td>0.079886</td>\n",
       "      <td>0.886552</td>\n",
       "      <td>0.687264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024918</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.886227</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>6.166344</td>\n",
       "      <td>0.977344</td>\n",
       "      <td>0.014307</td>\n",
       "      <td>0.497751</td>\n",
       "      <td>3.34795</td>\n",
       "      <td>0.031111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <th>10</th>\n",
       "      <td>0.183488</td>\n",
       "      <td>0.184813</td>\n",
       "      <td>0.038541</td>\n",
       "      <td>0.088173</td>\n",
       "      <td>0.088813</td>\n",
       "      <td>0.409281</td>\n",
       "      <td>0.195138</td>\n",
       "      <td>0.063702</td>\n",
       "      <td>0.824405</td>\n",
       "      <td>0.598424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.824103</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>5.856254</td>\n",
       "      <td>0.97583</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>0.472721</td>\n",
       "      <td>3.488069</td>\n",
       "      <td>0.030705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <th>10</th>\n",
       "      <td>0.196182</td>\n",
       "      <td>0.197855</td>\n",
       "      <td>0.041927</td>\n",
       "      <td>0.096849</td>\n",
       "      <td>0.097781</td>\n",
       "      <td>0.434544</td>\n",
       "      <td>0.209976</td>\n",
       "      <td>0.069089</td>\n",
       "      <td>0.847856</td>\n",
       "      <td>0.645641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.847546</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>4.598028</td>\n",
       "      <td>0.95385</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>0.371156</td>\n",
       "      <td>3.785153</td>\n",
       "      <td>0.030195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <th>10</th>\n",
       "      <td>0.216819</td>\n",
       "      <td>0.218645</td>\n",
       "      <td>0.047692</td>\n",
       "      <td>0.106216</td>\n",
       "      <td>0.107096</td>\n",
       "      <td>0.448492</td>\n",
       "      <td>0.227295</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.882301</td>\n",
       "      <td>0.687372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018772</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.881978</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>5.694162</td>\n",
       "      <td>0.971485</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.459636</td>\n",
       "      <td>3.489789</td>\n",
       "      <td>0.030845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <th>10</th>\n",
       "      <td>0.217472</td>\n",
       "      <td>0.21923</td>\n",
       "      <td>0.047768</td>\n",
       "      <td>0.105727</td>\n",
       "      <td>0.106503</td>\n",
       "      <td>0.444095</td>\n",
       "      <td>0.22676</td>\n",
       "      <td>0.078331</td>\n",
       "      <td>0.882594</td>\n",
       "      <td>0.682663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019049</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.882271</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>5.732572</td>\n",
       "      <td>0.971967</td>\n",
       "      <td>0.01029</td>\n",
       "      <td>0.462737</td>\n",
       "      <td>3.477107</td>\n",
       "      <td>0.03087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <th>10</th>\n",
       "      <td>0.19465</td>\n",
       "      <td>0.196312</td>\n",
       "      <td>0.041494</td>\n",
       "      <td>0.096452</td>\n",
       "      <td>0.097408</td>\n",
       "      <td>0.434752</td>\n",
       "      <td>0.209041</td>\n",
       "      <td>0.068405</td>\n",
       "      <td>0.843313</td>\n",
       "      <td>0.64455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.843004</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>4.540785</td>\n",
       "      <td>0.952723</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.366535</td>\n",
       "      <td>3.796031</td>\n",
       "      <td>0.030172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <th>10</th>\n",
       "      <td>0.195837</td>\n",
       "      <td>0.197525</td>\n",
       "      <td>0.041915</td>\n",
       "      <td>0.096819</td>\n",
       "      <td>0.09776</td>\n",
       "      <td>0.435068</td>\n",
       "      <td>0.209898</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.845804</td>\n",
       "      <td>0.646196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.845495</td>\n",
       "      <td>0.00116</td>\n",
       "      <td>4.583939</td>\n",
       "      <td>0.953578</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.370019</td>\n",
       "      <td>3.787859</td>\n",
       "      <td>0.03019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>10</th>\n",
       "      <td>0.195903</td>\n",
       "      <td>0.197299</td>\n",
       "      <td>0.042392</td>\n",
       "      <td>0.090787</td>\n",
       "      <td>0.09133</td>\n",
       "      <td>0.403149</td>\n",
       "      <td>0.201928</td>\n",
       "      <td>0.069701</td>\n",
       "      <td>0.852986</td>\n",
       "      <td>0.603018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039149</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.852674</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>7.520748</td>\n",
       "      <td>0.990627</td>\n",
       "      <td>0.034839</td>\n",
       "      <td>0.60708</td>\n",
       "      <td>2.874914</td>\n",
       "      <td>0.031944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <th>10</th>\n",
       "      <td>0.195178</td>\n",
       "      <td>0.196905</td>\n",
       "      <td>0.041707</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.097641</td>\n",
       "      <td>0.435312</td>\n",
       "      <td>0.209539</td>\n",
       "      <td>0.068727</td>\n",
       "      <td>0.844705</td>\n",
       "      <td>0.645796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.844396</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>4.550878</td>\n",
       "      <td>0.952986</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>0.36735</td>\n",
       "      <td>3.794245</td>\n",
       "      <td>0.030175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>10</th>\n",
       "      <td>0.216849</td>\n",
       "      <td>0.218652</td>\n",
       "      <td>0.047855</td>\n",
       "      <td>0.106217</td>\n",
       "      <td>0.107166</td>\n",
       "      <td>0.451424</td>\n",
       "      <td>0.227696</td>\n",
       "      <td>0.078407</td>\n",
       "      <td>0.883474</td>\n",
       "      <td>0.689399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01628</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.88315</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>5.753381</td>\n",
       "      <td>0.972915</td>\n",
       "      <td>0.009988</td>\n",
       "      <td>0.464417</td>\n",
       "      <td>3.483577</td>\n",
       "      <td>0.030763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>10</th>\n",
       "      <td>0.206596</td>\n",
       "      <td>0.20841</td>\n",
       "      <td>0.044814</td>\n",
       "      <td>0.102011</td>\n",
       "      <td>0.102984</td>\n",
       "      <td>0.442433</td>\n",
       "      <td>0.219219</td>\n",
       "      <td>0.073651</td>\n",
       "      <td>0.862807</td>\n",
       "      <td>0.668944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.862491</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>4.959552</td>\n",
       "      <td>0.961364</td>\n",
       "      <td>0.005661</td>\n",
       "      <td>0.400338</td>\n",
       "      <td>3.715399</td>\n",
       "      <td>0.030305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>10</th>\n",
       "      <td>0.207475</td>\n",
       "      <td>0.209169</td>\n",
       "      <td>0.045008</td>\n",
       "      <td>0.102397</td>\n",
       "      <td>0.103238</td>\n",
       "      <td>0.444262</td>\n",
       "      <td>0.219952</td>\n",
       "      <td>0.07397</td>\n",
       "      <td>0.864932</td>\n",
       "      <td>0.671446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00742</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.864615</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>5.009181</td>\n",
       "      <td>0.962102</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.404344</td>\n",
       "      <td>3.701576</td>\n",
       "      <td>0.030332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <th>10</th>\n",
       "      <td>0.212232</td>\n",
       "      <td>0.214011</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.104431</td>\n",
       "      <td>0.105242</td>\n",
       "      <td>0.447248</td>\n",
       "      <td>0.223814</td>\n",
       "      <td>0.076302</td>\n",
       "      <td>0.875266</td>\n",
       "      <td>0.680525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010466</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.874945</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>5.284196</td>\n",
       "      <td>0.966453</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.426544</td>\n",
       "      <td>3.628701</td>\n",
       "      <td>0.030472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>10</th>\n",
       "      <td>0.218666</td>\n",
       "      <td>0.220526</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>0.106869</td>\n",
       "      <td>0.107797</td>\n",
       "      <td>0.451576</td>\n",
       "      <td>0.229109</td>\n",
       "      <td>0.079381</td>\n",
       "      <td>0.886772</td>\n",
       "      <td>0.692319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022426</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.886447</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>5.99289</td>\n",
       "      <td>0.975398</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.48375</td>\n",
       "      <td>3.393099</td>\n",
       "      <td>0.031024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>10</th>\n",
       "      <td>0.215845</td>\n",
       "      <td>0.217688</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>0.106407</td>\n",
       "      <td>0.107226</td>\n",
       "      <td>0.448817</td>\n",
       "      <td>0.227002</td>\n",
       "      <td>0.07758</td>\n",
       "      <td>0.879296</td>\n",
       "      <td>0.688414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017664</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>5.643954</td>\n",
       "      <td>0.970927</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>0.455584</td>\n",
       "      <td>3.501947</td>\n",
       "      <td>0.030819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>10</th>\n",
       "      <td>0.197252</td>\n",
       "      <td>0.198871</td>\n",
       "      <td>0.042185</td>\n",
       "      <td>0.097512</td>\n",
       "      <td>0.098395</td>\n",
       "      <td>0.436485</td>\n",
       "      <td>0.211093</td>\n",
       "      <td>0.069505</td>\n",
       "      <td>0.849762</td>\n",
       "      <td>0.649409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.849451</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>4.635507</td>\n",
       "      <td>0.95478</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>0.374181</td>\n",
       "      <td>3.778117</td>\n",
       "      <td>0.030211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>10</th>\n",
       "      <td>0.217735</td>\n",
       "      <td>0.219596</td>\n",
       "      <td>0.048089</td>\n",
       "      <td>0.106047</td>\n",
       "      <td>0.106961</td>\n",
       "      <td>0.446615</td>\n",
       "      <td>0.227469</td>\n",
       "      <td>0.078779</td>\n",
       "      <td>0.882448</td>\n",
       "      <td>0.685379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021153</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.882125</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>5.931409</td>\n",
       "      <td>0.974752</td>\n",
       "      <td>0.012033</td>\n",
       "      <td>0.478787</td>\n",
       "      <td>3.414159</td>\n",
       "      <td>0.030985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <th>10</th>\n",
       "      <td>0.195837</td>\n",
       "      <td>0.197542</td>\n",
       "      <td>0.041897</td>\n",
       "      <td>0.096873</td>\n",
       "      <td>0.097799</td>\n",
       "      <td>0.435177</td>\n",
       "      <td>0.209905</td>\n",
       "      <td>0.069027</td>\n",
       "      <td>0.846024</td>\n",
       "      <td>0.646321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.845714</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>4.563886</td>\n",
       "      <td>0.953245</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>0.3684</td>\n",
       "      <td>3.791374</td>\n",
       "      <td>0.030182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <th>10</th>\n",
       "      <td>0.210473</td>\n",
       "      <td>0.212356</td>\n",
       "      <td>0.046143</td>\n",
       "      <td>0.103441</td>\n",
       "      <td>0.104347</td>\n",
       "      <td>0.446931</td>\n",
       "      <td>0.222421</td>\n",
       "      <td>0.075692</td>\n",
       "      <td>0.873653</td>\n",
       "      <td>0.67682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>5.247093</td>\n",
       "      <td>0.965855</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>0.423549</td>\n",
       "      <td>3.643363</td>\n",
       "      <td>0.030435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <th>10</th>\n",
       "      <td>0.216819</td>\n",
       "      <td>0.218573</td>\n",
       "      <td>0.047954</td>\n",
       "      <td>0.105419</td>\n",
       "      <td>0.106257</td>\n",
       "      <td>0.44409</td>\n",
       "      <td>0.226249</td>\n",
       "      <td>0.078538</td>\n",
       "      <td>0.8845</td>\n",
       "      <td>0.681589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017775</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.884176</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>5.964341</td>\n",
       "      <td>0.975772</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.481446</td>\n",
       "      <td>3.446382</td>\n",
       "      <td>0.030803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>10</th>\n",
       "      <td>0.215991</td>\n",
       "      <td>0.217712</td>\n",
       "      <td>0.047618</td>\n",
       "      <td>0.105635</td>\n",
       "      <td>0.106504</td>\n",
       "      <td>0.446624</td>\n",
       "      <td>0.226394</td>\n",
       "      <td>0.078032</td>\n",
       "      <td>0.880542</td>\n",
       "      <td>0.68427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014785</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.88022</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>5.751759</td>\n",
       "      <td>0.973463</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>3.500197</td>\n",
       "      <td>0.030699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <th>10</th>\n",
       "      <td>0.21635</td>\n",
       "      <td>0.218171</td>\n",
       "      <td>0.047387</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.107004</td>\n",
       "      <td>0.447697</td>\n",
       "      <td>0.226914</td>\n",
       "      <td>0.077746</td>\n",
       "      <td>0.879296</td>\n",
       "      <td>0.68636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018163</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>5.665158</td>\n",
       "      <td>0.971138</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0.457295</td>\n",
       "      <td>3.496643</td>\n",
       "      <td>0.030833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <th>10</th>\n",
       "      <td>0.213375</td>\n",
       "      <td>0.215209</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>0.104801</td>\n",
       "      <td>0.105704</td>\n",
       "      <td>0.446599</td>\n",
       "      <td>0.224569</td>\n",
       "      <td>0.076738</td>\n",
       "      <td>0.874606</td>\n",
       "      <td>0.681168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.874286</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>5.403671</td>\n",
       "      <td>0.968025</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.436188</td>\n",
       "      <td>3.590252</td>\n",
       "      <td>0.03057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <th>10</th>\n",
       "      <td>0.195698</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.041777</td>\n",
       "      <td>0.096885</td>\n",
       "      <td>0.097809</td>\n",
       "      <td>0.435485</td>\n",
       "      <td>0.209869</td>\n",
       "      <td>0.068856</td>\n",
       "      <td>0.845878</td>\n",
       "      <td>0.64656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.845568</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>4.562269</td>\n",
       "      <td>0.953204</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.368269</td>\n",
       "      <td>3.791579</td>\n",
       "      <td>0.030182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <th>10</th>\n",
       "      <td>0.186032</td>\n",
       "      <td>0.187372</td>\n",
       "      <td>0.038932</td>\n",
       "      <td>0.090047</td>\n",
       "      <td>0.090705</td>\n",
       "      <td>0.415206</td>\n",
       "      <td>0.198206</td>\n",
       "      <td>0.064389</td>\n",
       "      <td>0.826676</td>\n",
       "      <td>0.608729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.826374</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>5.793306</td>\n",
       "      <td>0.974919</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>0.467639</td>\n",
       "      <td>3.513957</td>\n",
       "      <td>0.030659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>10</th>\n",
       "      <td>0.166501</td>\n",
       "      <td>0.167987</td>\n",
       "      <td>0.036168</td>\n",
       "      <td>0.075158</td>\n",
       "      <td>0.075739</td>\n",
       "      <td>0.368601</td>\n",
       "      <td>0.173941</td>\n",
       "      <td>0.059427</td>\n",
       "      <td>0.801319</td>\n",
       "      <td>0.524714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033944</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.801026</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>7.858919</td>\n",
       "      <td>0.993117</td>\n",
       "      <td>0.040785</td>\n",
       "      <td>0.634377</td>\n",
       "      <td>2.720584</td>\n",
       "      <td>0.032118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <th>10</th>\n",
       "      <td>0.21399</td>\n",
       "      <td>0.215916</td>\n",
       "      <td>0.047136</td>\n",
       "      <td>0.104944</td>\n",
       "      <td>0.105858</td>\n",
       "      <td>0.446508</td>\n",
       "      <td>0.225115</td>\n",
       "      <td>0.077255</td>\n",
       "      <td>0.877831</td>\n",
       "      <td>0.682321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012791</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.877509</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>5.488021</td>\n",
       "      <td>0.969463</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.442997</td>\n",
       "      <td>3.567875</td>\n",
       "      <td>0.030593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <th>10</th>\n",
       "      <td>0.197362</td>\n",
       "      <td>0.199104</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>0.097562</td>\n",
       "      <td>0.098459</td>\n",
       "      <td>0.436331</td>\n",
       "      <td>0.211177</td>\n",
       "      <td>0.070049</td>\n",
       "      <td>0.850641</td>\n",
       "      <td>0.6493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.85033</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>4.693809</td>\n",
       "      <td>0.95586</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.378887</td>\n",
       "      <td>3.767847</td>\n",
       "      <td>0.030235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <th>10</th>\n",
       "      <td>0.210297</td>\n",
       "      <td>0.212183</td>\n",
       "      <td>0.046039</td>\n",
       "      <td>0.103584</td>\n",
       "      <td>0.104538</td>\n",
       "      <td>0.444903</td>\n",
       "      <td>0.222314</td>\n",
       "      <td>0.075541</td>\n",
       "      <td>0.871528</td>\n",
       "      <td>0.676282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.871209</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>5.201705</td>\n",
       "      <td>0.96522</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>0.419885</td>\n",
       "      <td>3.652884</td>\n",
       "      <td>0.030421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>10</th>\n",
       "      <td>0.162484</td>\n",
       "      <td>0.163912</td>\n",
       "      <td>0.034921</td>\n",
       "      <td>0.073382</td>\n",
       "      <td>0.073929</td>\n",
       "      <td>0.361515</td>\n",
       "      <td>0.169854</td>\n",
       "      <td>0.057486</td>\n",
       "      <td>0.790766</td>\n",
       "      <td>0.512976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027742</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>7.663252</td>\n",
       "      <td>0.992425</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>0.618583</td>\n",
       "      <td>2.798483</td>\n",
       "      <td>0.031906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <th>10</th>\n",
       "      <td>0.196607</td>\n",
       "      <td>0.198257</td>\n",
       "      <td>0.042084</td>\n",
       "      <td>0.097145</td>\n",
       "      <td>0.098022</td>\n",
       "      <td>0.435014</td>\n",
       "      <td>0.210374</td>\n",
       "      <td>0.069329</td>\n",
       "      <td>0.847343</td>\n",
       "      <td>0.647077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.847033</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>4.612573</td>\n",
       "      <td>0.954164</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.37233</td>\n",
       "      <td>3.782136</td>\n",
       "      <td>0.030202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>10</th>\n",
       "      <td>0.210194</td>\n",
       "      <td>0.212049</td>\n",
       "      <td>0.04602</td>\n",
       "      <td>0.103938</td>\n",
       "      <td>0.104873</td>\n",
       "      <td>0.449106</td>\n",
       "      <td>0.22288</td>\n",
       "      <td>0.075508</td>\n",
       "      <td>0.870429</td>\n",
       "      <td>0.680228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.87011</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>5.203906</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.006652</td>\n",
       "      <td>0.420063</td>\n",
       "      <td>3.65242</td>\n",
       "      <td>0.030416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>10</th>\n",
       "      <td>0.206647</td>\n",
       "      <td>0.208439</td>\n",
       "      <td>0.044705</td>\n",
       "      <td>0.102049</td>\n",
       "      <td>0.103007</td>\n",
       "      <td>0.442733</td>\n",
       "      <td>0.219241</td>\n",
       "      <td>0.073507</td>\n",
       "      <td>0.86376</td>\n",
       "      <td>0.66915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.863443</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>4.941911</td>\n",
       "      <td>0.961075</td>\n",
       "      <td>0.005586</td>\n",
       "      <td>0.398914</td>\n",
       "      <td>3.719504</td>\n",
       "      <td>0.030297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "   cutoff                                                                      \n",
       "0  10      0.216702                 0.218582   0.04796   0.10627    0.107255   \n",
       "1  10      0.198051                 0.199712  0.042484  0.097737    0.098644   \n",
       "2  10       0.19528                  0.19696  0.041702  0.096592    0.097548   \n",
       "3  10      0.194899                 0.196596  0.041582   0.09652    0.097471   \n",
       "4  10      0.211513                 0.213258  0.046039  0.104431    0.105312   \n",
       "5  10      0.213983                   0.2158  0.047066  0.104902    0.105802   \n",
       "6  10      0.195075                 0.196788  0.041639  0.096547     0.09751   \n",
       "7  10      0.209036                 0.210734  0.045319  0.099735    0.100452   \n",
       "8  10      0.195346                 0.197013    0.0417  0.096606    0.097541   \n",
       "9  10      0.200007                 0.201718  0.043175  0.098506    0.099355   \n",
       "10 10      0.194936                 0.196647  0.041631  0.096509    0.097463   \n",
       "11 10      0.207952                 0.209632   0.04509  0.099576    0.100233   \n",
       "12 10      0.209527                 0.211256  0.045435  0.103507    0.104434   \n",
       "13 10      0.207409                 0.209152  0.044869  0.102473    0.103445   \n",
       "14 10      0.209718                 0.211404  0.045512  0.103244    0.104119   \n",
       "15 10       0.20911                 0.210995  0.045781  0.103278    0.104252   \n",
       "16 10      0.217435                 0.219206  0.047885  0.105902    0.106684   \n",
       "17 10      0.219384                  0.22123  0.048834  0.106602    0.107462   \n",
       "18 10      0.183488                 0.184813  0.038541  0.088173    0.088813   \n",
       "19 10      0.196182                 0.197855  0.041927  0.096849    0.097781   \n",
       "20 10      0.216819                 0.218645  0.047692  0.106216    0.107096   \n",
       "21 10      0.217472                  0.21923  0.047768  0.105727    0.106503   \n",
       "22 10       0.19465                 0.196312  0.041494  0.096452    0.097408   \n",
       "23 10      0.195837                 0.197525  0.041915  0.096819     0.09776   \n",
       "24 10      0.195903                 0.197299  0.042392  0.090787     0.09133   \n",
       "25 10      0.195178                 0.196905  0.041707  0.096667    0.097641   \n",
       "26 10      0.216849                 0.218652  0.047855  0.106217    0.107166   \n",
       "27 10      0.206596                  0.20841  0.044814  0.102011    0.102984   \n",
       "28 10      0.207475                 0.209169  0.045008  0.102397    0.103238   \n",
       "29 10      0.212232                 0.214011  0.046512  0.104431    0.105242   \n",
       "30 10      0.218666                 0.220526  0.048493  0.106869    0.107797   \n",
       "31 10      0.215845                 0.217688  0.047289  0.106407    0.107226   \n",
       "32 10      0.197252                 0.198871  0.042185  0.097512    0.098395   \n",
       "33 10      0.217735                 0.219596  0.048089  0.106047    0.106961   \n",
       "34 10      0.195837                 0.197542  0.041897  0.096873    0.097799   \n",
       "35 10      0.210473                 0.212356  0.046143  0.103441    0.104347   \n",
       "36 10      0.216819                 0.218573  0.047954  0.105419    0.106257   \n",
       "37 10      0.215991                 0.217712  0.047618  0.105635    0.106504   \n",
       "38 10       0.21635                 0.218171  0.047387    0.1061    0.107004   \n",
       "39 10      0.213375                 0.215209  0.046781  0.104801    0.105704   \n",
       "40 10      0.195698                 0.197368  0.041777  0.096885    0.097809   \n",
       "41 10      0.186032                 0.187372  0.038932  0.090047    0.090705   \n",
       "42 10      0.166501                 0.167987  0.036168  0.075158    0.075739   \n",
       "43 10       0.21399                 0.215916  0.047136  0.104944    0.105858   \n",
       "44 10      0.197362                 0.199104  0.042581  0.097562    0.098459   \n",
       "45 10      0.210297                 0.212183  0.046039  0.103584    0.104538   \n",
       "46 10      0.162484                 0.163912  0.034921  0.073382    0.073929   \n",
       "47 10      0.196607                 0.198257  0.042084  0.097145    0.098022   \n",
       "48 10      0.210194                 0.212049   0.04602  0.103938    0.104873   \n",
       "49 10      0.206647                 0.208439  0.044705  0.102049    0.103007   \n",
       "\n",
       "                MRR      NDCG        F1  HIT_RATE ARHR_ALL_HITS  ...  \\\n",
       "   cutoff                                                        ...   \n",
       "0  10      0.451879  0.227804  0.078539  0.884207      0.690058  ...   \n",
       "1  10      0.436063  0.211581  0.069961  0.851228      0.649777  ...   \n",
       "2  10      0.433894  0.209346  0.068727  0.845071      0.644415  ...   \n",
       "3  10      0.434693  0.209191   0.06854  0.844119       0.64473  ...   \n",
       "4  10      0.448648  0.223676  0.075619  0.873067      0.681465  ...   \n",
       "5  10      0.448261  0.225132   0.07716  0.880322      0.682769  ...   \n",
       "6  10        0.4344  0.209266  0.068629  0.843313       0.64457  ...   \n",
       "7  10      0.425312  0.216558  0.074489  0.864492       0.64878  ...   \n",
       "8  10      0.434797  0.209464  0.068729  0.845071       0.64515  ...   \n",
       "9  10      0.437053  0.213121   0.07102  0.853939      0.653221  ...   \n",
       "10 10      0.434455  0.209166  0.068609  0.843679      0.644427  ...   \n",
       "11 10      0.424614  0.215836  0.074111  0.861781       0.64803  ...   \n",
       "12 10      0.446876  0.221972  0.074677  0.869476      0.677103  ...   \n",
       "13 10      0.444252  0.220052  0.073777  0.866325      0.671676  ...   \n",
       "14 10      0.444162  0.221585  0.074793  0.868597      0.674305  ...   \n",
       "15 10      0.446462  0.221753  0.075116  0.869623        0.6764  ...   \n",
       "16 10      0.444706  0.227002  0.078486  0.882814      0.684076  ...   \n",
       "17 10      0.446143  0.228548  0.079886  0.886552      0.687264  ...   \n",
       "18 10      0.409281  0.195138  0.063702  0.824405      0.598424  ...   \n",
       "19 10      0.434544  0.209976  0.069089  0.847856      0.645641  ...   \n",
       "20 10      0.448492  0.227295  0.078186  0.882301      0.687372  ...   \n",
       "21 10      0.444095   0.22676  0.078331  0.882594      0.682663  ...   \n",
       "22 10      0.434752  0.209041  0.068405  0.843313       0.64455  ...   \n",
       "23 10      0.435068  0.209898   0.06905  0.845804      0.646196  ...   \n",
       "24 10      0.403149  0.201928  0.069701  0.852986      0.603018  ...   \n",
       "25 10      0.435312  0.209539  0.068727  0.844705      0.645796  ...   \n",
       "26 10      0.451424  0.227696  0.078407  0.883474      0.689399  ...   \n",
       "27 10      0.442433  0.219219  0.073651  0.862807      0.668944  ...   \n",
       "28 10      0.444262  0.219952   0.07397  0.864932      0.671446  ...   \n",
       "29 10      0.447248  0.223814  0.076302  0.875266      0.680525  ...   \n",
       "30 10      0.451576  0.229109  0.079381  0.886772      0.692319  ...   \n",
       "31 10      0.448817  0.227002   0.07758  0.879296      0.688414  ...   \n",
       "32 10      0.436485  0.211093  0.069505  0.849762      0.649409  ...   \n",
       "33 10      0.446615  0.227469  0.078779  0.882448      0.685379  ...   \n",
       "34 10      0.435177  0.209905  0.069027  0.846024      0.646321  ...   \n",
       "35 10      0.446931  0.222421  0.075692  0.873653       0.67682  ...   \n",
       "36 10       0.44409  0.226249  0.078538    0.8845      0.681589  ...   \n",
       "37 10      0.446624  0.226394  0.078032  0.880542       0.68427  ...   \n",
       "38 10      0.447697  0.226914  0.077746  0.879296       0.68636  ...   \n",
       "39 10      0.446599  0.224569  0.076738  0.874606      0.681168  ...   \n",
       "40 10      0.435485  0.209869  0.068856  0.845878       0.64656  ...   \n",
       "41 10      0.415206  0.198206  0.064389  0.826676      0.608729  ...   \n",
       "42 10      0.368601  0.173941  0.059427  0.801319      0.524714  ...   \n",
       "43 10      0.446508  0.225115  0.077255  0.877831      0.682321  ...   \n",
       "44 10      0.436331  0.211177  0.070049  0.850641        0.6493  ...   \n",
       "45 10      0.444903  0.222314  0.075541  0.871528      0.676282  ...   \n",
       "46 10      0.361515  0.169854  0.057486  0.790766      0.512976  ...   \n",
       "47 10      0.435014  0.210374  0.069329  0.847343      0.647077  ...   \n",
       "48 10      0.449106   0.22288  0.075508  0.870429      0.680228  ...   \n",
       "49 10      0.442733  0.219241  0.073507   0.86376       0.66915  ...   \n",
       "\n",
       "          COVERAGE_ITEM_CORRECT COVERAGE_USER COVERAGE_USER_CORRECT  \\\n",
       "   cutoff                                                             \n",
       "0  10                  0.016446      0.999634              0.883883   \n",
       "1  10                  0.003267      0.999634              0.850916   \n",
       "2  10                  0.002436      0.999634              0.844762   \n",
       "3  10                  0.002381      0.999634               0.84381   \n",
       "4  10                  0.010189      0.999634              0.872747   \n",
       "5  10                  0.012072      0.999634                  0.88   \n",
       "6  10                  0.002381      0.999634              0.843004   \n",
       "7  10                  0.016834      0.999634              0.864176   \n",
       "8  10                  0.002492      0.999634              0.844762   \n",
       "9  10                  0.004984      0.999634              0.853626   \n",
       "10 10                  0.002381      0.999634               0.84337   \n",
       "11 10                   0.01844      0.999634              0.861465   \n",
       "12 10                  0.009026      0.999634              0.869158   \n",
       "13 10                  0.007033      0.999634              0.866007   \n",
       "14 10                  0.009026      0.999634              0.868278   \n",
       "15 10                  0.008749      0.999634              0.869304   \n",
       "16 10                  0.020101      0.999634              0.882491   \n",
       "17 10                  0.024918      0.999634              0.886227   \n",
       "18 10                  0.008361      0.999634              0.824103   \n",
       "19 10                  0.002935      0.999634              0.847546   \n",
       "20 10                  0.018772      0.999634              0.881978   \n",
       "21 10                  0.019049      0.999634              0.882271   \n",
       "22 10                  0.002381      0.999634              0.843004   \n",
       "23 10                  0.002769      0.999634              0.845495   \n",
       "24 10                  0.039149      0.999634              0.852674   \n",
       "25 10                  0.002436      0.999634              0.844396   \n",
       "26 10                   0.01628      0.999634               0.88315   \n",
       "27 10                  0.006534      0.999634              0.862491   \n",
       "28 10                   0.00742      0.999634              0.864615   \n",
       "29 10                  0.010466      0.999634              0.874945   \n",
       "30 10                  0.022426      0.999634              0.886447   \n",
       "31 10                  0.017664      0.999634              0.878974   \n",
       "32 10                  0.003267      0.999634              0.849451   \n",
       "33 10                  0.021153      0.999634              0.882125   \n",
       "34 10                  0.002547      0.999634              0.845714   \n",
       "35 10                  0.010189      0.999634              0.873333   \n",
       "36 10                  0.017775      0.999634              0.884176   \n",
       "37 10                  0.014785      0.999634               0.88022   \n",
       "38 10                  0.018163      0.999634              0.878974   \n",
       "39 10                  0.012958      0.999634              0.874286   \n",
       "40 10                  0.002603      0.999634              0.845568   \n",
       "41 10                  0.008306      0.999634              0.826374   \n",
       "42 10                  0.033944      0.999634              0.801026   \n",
       "43 10                  0.012791      0.999634              0.877509   \n",
       "44 10                  0.004042      0.999634               0.85033   \n",
       "45 10                  0.009524      0.999634              0.871209   \n",
       "46 10                  0.027742      0.999634              0.790476   \n",
       "47 10                  0.003101      0.999634              0.847033   \n",
       "48 10                  0.009247      0.999634               0.87011   \n",
       "49 10                  0.006313      0.999634              0.863443   \n",
       "\n",
       "          DIVERSITY_GINI SHANNON_ENTROPY RATIO_DIVERSITY_HERFINDAHL  \\\n",
       "   cutoff                                                             \n",
       "0  10           0.002433        5.731718                   0.972642   \n",
       "1  10            0.00121        4.649879                   0.955118   \n",
       "2  10           0.001133        4.555582                   0.953094   \n",
       "3  10           0.001123        4.544407                   0.952807   \n",
       "4  10           0.001676         5.23377                   0.965609   \n",
       "5  10            0.00208        5.534669                   0.970676   \n",
       "6  10           0.001126        4.547455                   0.952883   \n",
       "7  10           0.003391        6.265358                   0.980976   \n",
       "8  10           0.001134        4.557139                   0.953081   \n",
       "9  10           0.001428        4.887093                   0.959726   \n",
       "10 10           0.001125        4.546945                   0.952868   \n",
       "11 10           0.003691        6.390841                   0.982446   \n",
       "12 10           0.001539        5.108489                   0.963717   \n",
       "13 10           0.001427        4.991772                   0.961863   \n",
       "14 10           0.001563        5.127738                   0.964038   \n",
       "15 10           0.001577        5.138025                   0.964181   \n",
       "16 10           0.002704        5.795739                   0.972831   \n",
       "17 10           0.003542        6.166344                   0.977344   \n",
       "18 10           0.002603        5.856254                    0.97583   \n",
       "19 10           0.001167        4.598028                    0.95385   \n",
       "20 10           0.002464        5.694162                   0.971485   \n",
       "21 10           0.002547        5.732572                   0.971967   \n",
       "22 10            0.00112        4.540785                   0.952723   \n",
       "23 10            0.00116        4.583939                   0.953578   \n",
       "24 10           0.008625        7.520748                   0.990627   \n",
       "25 10           0.001128        4.550878                   0.952986   \n",
       "26 10           0.002473        5.753381                   0.972915   \n",
       "27 10           0.001402        4.959552                   0.961364   \n",
       "28 10           0.001453        5.009181                   0.962102   \n",
       "29 10           0.001748        5.284196                   0.966453   \n",
       "30 10           0.003125         5.99289                   0.975398   \n",
       "31 10           0.002348        5.643954                   0.970927   \n",
       "32 10           0.001203        4.635507                    0.95478   \n",
       "33 10           0.002979        5.931409                   0.974752   \n",
       "34 10           0.001141        4.563886                   0.953245   \n",
       "35 10           0.001709        5.247093                   0.965855   \n",
       "36 10           0.002872        5.964341                   0.975772   \n",
       "37 10           0.002437        5.751759                   0.973463   \n",
       "38 10           0.002403        5.665158                   0.971138   \n",
       "39 10           0.001908        5.403671                   0.968025   \n",
       "40 10            0.00114        4.562269                   0.953204   \n",
       "41 10           0.002504        5.793306                   0.974919   \n",
       "42 10           0.010097        7.858919                   0.993117   \n",
       "43 10           0.002019        5.488021                   0.969463   \n",
       "44 10           0.001263        4.693809                    0.95586   \n",
       "45 10           0.001647        5.201705                    0.96522   \n",
       "46 10           0.008648        7.663252                   0.992425   \n",
       "47 10           0.001179        4.612573                   0.954164   \n",
       "48 10           0.001647        5.203906                   0.965392   \n",
       "49 10           0.001383        4.941911                   0.961075   \n",
       "\n",
       "          RATIO_DIVERSITY_GINI RATIO_SHANNON_ENTROPY RATIO_AVERAGE_POPULARITY  \\\n",
       "   cutoff                                                                       \n",
       "0  10                 0.009829              0.462668                 3.488735   \n",
       "1  10                 0.004888              0.375341                 3.776113   \n",
       "2  10                 0.004576              0.367729                 3.793276   \n",
       "3  10                 0.004536              0.366828                 3.795377   \n",
       "4  10                 0.006771              0.422473                 3.640208   \n",
       "5  10                 0.008401              0.446762                 3.573563   \n",
       "6  10                 0.004547              0.367074                 3.794919   \n",
       "7  10                 0.013698              0.505744                 3.355526   \n",
       "8  10                 0.004581              0.367855                  3.79281   \n",
       "9  10                 0.005769              0.394489                  3.73683   \n",
       "10 10                 0.004543              0.367032                 3.795023   \n",
       "11 10                  0.01491              0.515873                 3.308902   \n",
       "12 10                 0.006215              0.412361                 3.677017   \n",
       "13 10                 0.005764              0.402939                  3.70647   \n",
       "14 10                 0.006314              0.413914                 3.673055   \n",
       "15 10                 0.006371              0.414745                 3.670592   \n",
       "16 10                 0.010924              0.467836                  3.45587   \n",
       "17 10                 0.014307              0.497751                  3.34795   \n",
       "18 10                 0.010514              0.472721                 3.488069   \n",
       "19 10                 0.004712              0.371156                 3.785153   \n",
       "20 10                 0.009953              0.459636                 3.489789   \n",
       "21 10                  0.01029              0.462737                 3.477107   \n",
       "22 10                 0.004524              0.366535                 3.796031   \n",
       "23 10                 0.004684              0.370019                 3.787859   \n",
       "24 10                 0.034839               0.60708                 2.874914   \n",
       "25 10                 0.004556               0.36735                 3.794245   \n",
       "26 10                 0.009988              0.464417                 3.483577   \n",
       "27 10                 0.005661              0.400338                 3.715399   \n",
       "28 10                 0.005868              0.404344                 3.701576   \n",
       "29 10                 0.007061              0.426544                 3.628701   \n",
       "30 10                 0.012621               0.48375                 3.393099   \n",
       "31 10                 0.009482              0.455584                 3.501947   \n",
       "32 10                 0.004858              0.374181                 3.778117   \n",
       "33 10                 0.012033              0.478787                 3.414159   \n",
       "34 10                 0.004609                0.3684                 3.791374   \n",
       "35 10                 0.006901              0.423549                 3.643363   \n",
       "36 10                 0.011598              0.481446                 3.446382   \n",
       "37 10                 0.009843              0.464286                 3.500197   \n",
       "38 10                 0.009707              0.457295                 3.496643   \n",
       "39 10                 0.007706              0.436188                 3.590252   \n",
       "40 10                 0.004603              0.368269                 3.791579   \n",
       "41 10                 0.010114              0.467639                 3.513957   \n",
       "42 10                 0.040785              0.634377                 2.720584   \n",
       "43 10                 0.008155              0.442997                 3.567875   \n",
       "44 10                 0.005101              0.378887                 3.767847   \n",
       "45 10                 0.006654              0.419885                 3.652884   \n",
       "46 10                 0.034929              0.618583                 2.798483   \n",
       "47 10                 0.004763               0.37233                 3.782136   \n",
       "48 10                 0.006652              0.420063                  3.65242   \n",
       "49 10                 0.005586              0.398914                 3.719504   \n",
       "\n",
       "          RATIO_NOVELTY  \n",
       "   cutoff                \n",
       "0  10          0.030756  \n",
       "1  10          0.030215  \n",
       "2  10          0.030177  \n",
       "3  10          0.030173  \n",
       "4  10          0.030463  \n",
       "5  10          0.030552  \n",
       "6  10          0.030174  \n",
       "7  10          0.030893  \n",
       "8  10          0.030179  \n",
       "9  10          0.030289  \n",
       "10 10          0.030174  \n",
       "11 10          0.030971  \n",
       "12 10          0.030383  \n",
       "13 10          0.030324  \n",
       "14 10          0.030387  \n",
       "15 10          0.030387  \n",
       "16 10          0.030911  \n",
       "17 10          0.031111  \n",
       "18 10          0.030705  \n",
       "19 10          0.030195  \n",
       "20 10          0.030845  \n",
       "21 10           0.03087  \n",
       "22 10          0.030172  \n",
       "23 10           0.03019  \n",
       "24 10          0.031944  \n",
       "25 10          0.030175  \n",
       "26 10          0.030763  \n",
       "27 10          0.030305  \n",
       "28 10          0.030332  \n",
       "29 10          0.030472  \n",
       "30 10          0.031024  \n",
       "31 10          0.030819  \n",
       "32 10          0.030211  \n",
       "33 10          0.030985  \n",
       "34 10          0.030182  \n",
       "35 10          0.030435  \n",
       "36 10          0.030803  \n",
       "37 10          0.030699  \n",
       "38 10          0.030833  \n",
       "39 10           0.03057  \n",
       "40 10          0.030182  \n",
       "41 10          0.030659  \n",
       "42 10          0.032118  \n",
       "43 10          0.030593  \n",
       "44 10          0.030235  \n",
       "45 10          0.030421  \n",
       "46 10          0.031906  \n",
       "47 10          0.030202  \n",
       "48 10          0.030416  \n",
       "49 10          0.030297  \n",
       "\n",
       "[50 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_on_validation_df = search_metadata[\"result_on_validation_df\"]\n",
    "result_on_validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 13650 (100.0%), 506.29 column/sec. Elapsed time 26.96 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 14.86 sec. Users per second: 918\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 13650 (100.0%), 575.99 column/sec. Elapsed time 23.70 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 16.43 sec. Users per second: 831\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 13650 (100.0%), 573.50 column/sec. Elapsed time 23.80 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 17.57 sec. Users per second: 777\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 13650 (100.0%), 571.12 column/sec. Elapsed time 23.90 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 18.93 sec. Users per second: 721\n"
     ]
    }
   ],
   "source": [
    "from Recommenders.KNN.UserKNNCFRecommender import UserKNNCFRecommender\n",
    "\n",
    "x_tick = [400, 500, 600, 700]\n",
    "MAP_per_k = []\n",
    "\n",
    "for topK in x_tick:\n",
    "    \n",
    "    recommender = UserKNNCFRecommender(URM_train)\n",
    "    recommender.fit(shrink=0.0, topK=topK)\n",
    "    \n",
    "    result_df, _ = evaluator_validation.evaluateRecommender(recommender)\n",
    "    \n",
    "    MAP_per_k.append(result_df.loc[10][\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 13650 (100.0%), 517.66 column/sec. Elapsed time 26.37 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 16.15 sec. Users per second: 845\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 13650 (100.0%), 584.88 column/sec. Elapsed time 23.34 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 16.10 sec. Users per second: 848\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 13650 (100.0%), 584.33 column/sec. Elapsed time 23.36 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 16.15 sec. Users per second: 845\n",
      "Unable to load Cython Compute_Similarity, reverting to Python\n",
      "Similarity column 13650 (100.0%), 582.46 column/sec. Elapsed time 23.44 sec\n",
      "EvaluatorHoldout: Processed 13645 (100.0%) in 16.22 sec. Users per second: 841\n"
     ]
    }
   ],
   "source": [
    "from Recommenders.KNN.UserKNNCFRecommender import UserKNNCFRecommender\n",
    "\n",
    "x_tick = [0.0, 1.0, 5.0, 10.0]\n",
    "MAP_per_k = []\n",
    "\n",
    "for topK in x_tick:\n",
    "    \n",
    "    recommender = UserKNNCFRecommender(URM_train)\n",
    "    recommender.fit(shrink=topK, topK=500)\n",
    "    \n",
    "    result_df, _ = evaluator_validation.evaluateRecommender(recommender)\n",
    "    \n",
    "    MAP_per_k.append(result_df.loc[10][\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.plot(x_tick, MAP_per_k)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('TopK')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
